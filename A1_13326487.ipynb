{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_13326487.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackpearl231/UTS_ML_13326487/blob/master/A1_13326487.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw6DJonKEIWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qQZugc4EJ3s",
        "colab_type": "text"
      },
      "source": [
        "# Generative Adversarial Nets\n",
        "# Assignment 1\n",
        "done by:\n",
        "\n",
        "**Krut Kanjiya**\n",
        "\n",
        "**13326487**\n",
        "\n",
        "available at https://github.com/blackpearl231/UTS_ML_13326487/blob/master/A1_13326487.ipynb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1. Introduction\n",
        "\n",
        "\n",
        "\n",
        "This paper is about Generative Adversarial Nets, also known as GAN.  GAN is a part of generative models. These generative models can have two types of density. It can be either an Explicit density or an implicit density. Explicit density is further categorized as Tractable density and Approximate density. Pixel RNN and Pixel CNN have Tractable Density probability distribution If we further categorize approximate density, subcategories will be Variational and Markov Chain.  Variational Autoencoder is a part of Variational density distribution while the Boltzmann machine follows the Markov chain. Implicit Density is categorized as Direct and Markov chain. These are represented by GAN and GSN, respectively.\n",
        "\n",
        "This paper has a complete understanding of GAN and how does it work.\n",
        "\n",
        "\n",
        "![alt text](https://cdn1.imggmi.com/uploads/2019/8/28/299077f6620fe8733ac9ba61033a3992-full.jpg)\n",
        "\n",
        "\n",
        "# Content\n",
        "\n",
        "  This paper describes the Generative Adversarial Nets. Generative Adversarial Net is a generative model which has implicit probability density. There are two models involved. One is the Generative model, and the other is Discriminative model. In simple language, the Generative model is described as a counterfeiter, and the Discriminative model is the police. Counterfeiters will try to produce fake currency and will try to use it without any detection, and on the other hand, Police will try to detect fake currency. So when the police stop detecting the fake currencies, the model is successfully trained. This is one of the special cases known as Adversarial Nets, where generative models generate a sample by using random noise and passing it through multilayer perceptron. Here, discriminative models are also multilayer perceptron. They have provided a section about related work, which describes many terms which should be known to you to understand this paper.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "Coming on to the main topic, Adversarial Nets, Authors have started describing everything in mathematical terms so it would be easy to calculate, understand, and predict from that. z is a random noise variable, and by using it, the generative model is going to generate samples. Now mapping between z and samples is represented by G(z,θg), while, D(x,θd) represents that the x came from data rather than these generated samples. D should maximize the probability of assigning labels to the training samples as well as generated samples. We parallelly train G to minimize a function log(1 - D(G(z))). By doing this, we can achieve good results. Apart from this, authors have proposed an algorithm for training this model and have proposed a theorem and two prepositions along with their proof. They have experimented by training MNIST, the Toronto Face Database (TFD), and CIFAR-10 datasets with different models and predicted log-likelihood along with standard errors. Also, difficulties faced by different approaches to deep generative modeling is mentioned. There are advantages and disadvantages mentioned, and in the end, they concluded and also provided some future work.\n",
        "\n",
        "\n",
        "# Innovation\n",
        "\n",
        "This paper on GAN is very innovative in many senses. Starting with this model is able to overcome many limitations which the other model had. Like, previous generative models were not able to calculate the probability distribution for the dataset. They would depend on an approximation to provide the estimated p(x), which would affect the quality of the training provided to generative models. In GAN, the generative model does not need any data input in order to generate samples while it will use random noise to generate samples, and that’s how it is completely independent. Another thing, when it came to minimizing G, instead of minimizing function log(1 - D(G(z))), they chose to maximize log D(G(z)). That was a smart choice. Also, the way they have derived proofs of theorems and prepositions, those are also unique. \n",
        "\n",
        "\n",
        "\n",
        "They have done experiments of various datasets and have described the results properly. These datasets are MNIST, TFD, and CIFAR-10 they are very diverse from each other. So this experiment covers a lot of ground like how is it going to work on different kind of datasets. They have also given information if you use any different approaches than this then how your result will get affected and also in which part, and what kind of difficulties will come into the picture. Besides all these, they have given this information in tabular format, which I would say well presented. Using GAN will reduce a whole lot of computational resource, and this is one of the major benefits.\n",
        "\n",
        "\n",
        "At last, I would say that sometimes, it is not enough to just read and understand everything. Practical knowledge is also required. Considering this, Authors have given a link to their GitHub repository so, just in case someone wants to try it and observe themselves or make some change and bring their own innovation, and this is absolutely the best thing about this paper.  \n",
        "\n",
        "# Technical Quality\n",
        "\n",
        "\n",
        "This paper is technically very sound. The theoretical part is well explained, and mathematical calculations and use of statistics very well support the outcomes. Therefore, readers not only can understand how it works but also can detect any flaws as well as suggests some improvements. To fully understand this paper, the reader should have to have good knowledge of machine learning, probability, and statistics. They have provided a separate section of related work which consists of all the terms related to the topic of the paper. Generative model G is a function that provides a mapping from random noise to generate samples, and Discriminative model D relates whether the samples are coming from the dataset rather than the generated ones. Now our goal is to maximize D and minimize G, in order to do that, readers need to have concepts of derivatives as well. Above all this, authors have stated an algorithm and proposed a theorem and described results by stating two of prepositions. These things are proved with basic mathematics with minimal complexity. That makes it easier to understand to readers. They have experimented using a variety of datasets. And they have come to a result of practically training the algorithms. And also, they have provided the link to its GitHub repository so if any readers would like to check their work; they can directly use it and test different scenario. At last, I would say that the authors did a good job to maintain the technicality of the report.\n",
        "\n",
        "\n",
        "# Application and X-factor\n",
        "\n",
        "A generative model with an Adversarial net can be a huge step up to machine learning algorithms. Using GAN, many things could be achieved, which was not possible earlier. The major use of Generative Adversarial Nets is to create Deepfakes. Deepfakes are known as fake photos or videos (mostly they are of a famous person or a celebrity, and many of the times they are being used to defame them). So if GAN creates it, then the generative model will keep generating photos or videos, until the discriminative model is not able to distinguish it from the real face. It can also train a voice sample. Apart from Deepfakes, GAN can be used by an artist as well because by changing the values in noise parameter; you can generate different outputs that you want, and it can be used for creative purposes. \n",
        "\n",
        "\n",
        "Future work, to be done on this topic can be related to Optimization of the algorithms or can be related to the performance increase. The authors have suggested that efficiency can be increased through the development of better methods for training the generative and discriminative model as well as better methods for sampling from the distribution. By using Semi-supervised learning, features from the discriminator could improve the performance of classifiers when the availability of labeled data is limited. Also, a conditional generative model can be obtained by adding input to both D and G. Ultimately, I would say that there are a lot of scopes of the Generative Adversarial Nets.\n",
        "\n",
        "\n",
        "# Presentation\n",
        "\n",
        "The abstract of this paper was very informative; therefore, readers could easily understand.  The introduction contains some nontechnical explanation as well, so everyone should be able to understand it. Structure wise, they have provided an additional section of related work before starting the actual content. Therefore it will give readers some information about its relation with other topics or work. After that, Adversarial Net has been explained and an algorithm to train the model followed by a theorem and two prepositions along with their proofs are also given. They have provided different section about the experiments they conducted on a variety of datasets and results are mentioned clearly. Based on the experiment conducted on different models, a table is given regarding different characteristics that gives a fair comparison between all the models. Before concluding the article, the authors have stated the Advantages and disadvantages of GAN. And at last, they have provided work to be done in the future along with a conclusion. They have also mentioned Acknowledgements at the end followed by references. Theories apart, they have mentioned the link to their GitHub repository. Therefore, people can see for themselves how the model is trained, or the algorithm works.\n",
        "\n",
        "# References\n",
        "\n",
        "\n",
        "*   Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y., 2014. Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680).\n",
        "\n",
        "\n"
      ]
    }
  ]
}